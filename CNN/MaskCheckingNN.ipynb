{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskCheckingNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1IKglu6Sf4r"
      },
      "source": [
        "#FaceMask detection model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW7eMmb1Ssaj"
      },
      "source": [
        "**Build your CNN model using Keras to identify whether a person is wearing a faceMask or not**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FBX03tXTDJ4"
      },
      "source": [
        "###**1**.   **The Setup phase**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk2SJem1TCfv"
      },
      "source": [
        "#---------------------------The import statments-----------------------------------------\n",
        "\n",
        "import pandas as pd                               #reading, writing and manipulating the data (using tables)\n",
        "import numpy as np                                #Library for linear algebra and some probabiltity (raw data) \n",
        "import tensorflow as tf                           #library for numerical computation that makes machine learning faster and easier\n",
        "from tensorflow import keras                      #we may not add this since keras is already thier in tensorflow\n",
        "from tensorflow.keras.models import Sequential    #To create the sequential layer\n",
        "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D,Dropout  #To create the model\n",
        "from tensorflow.keras.optimizers import Adam      #Adam optimizer\n",
        "from keras.preprocessing import image             #used for image classification\n",
        "from keras.preprocessing.image import ImageDataGenerator  #used to expand the training dataset in order to improve the performance and ability of the model to generalize\n",
        "import matplotlib.pyplot as plt                   #library to plot graphs\n",
        "from google.colab import files                    #To be able to upload files"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih6T8eYyZOP3"
      },
      "source": [
        "Here I will upload the dataset file called `\"data\"` that has 2 subfolders `\"with\"` and `\"without`\" and unzip\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXYnJTh8XTMY"
      },
      "source": [
        "#upload file\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VrTw-gqOZAq"
      },
      "source": [
        "#unzipping the folder\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8uYWBjW0BXw"
      },
      "source": [
        "#unzipping the folder\n",
        "!unzip test-70x70.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ET7hWGKfPB-"
      },
      "source": [
        "#delete the zip file as it is not needed anymore\n",
        "!rm data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbgEGNaw0Ymw"
      },
      "source": [
        "!rm test-70x70.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkjRtDw0Hhu4"
      },
      "source": [
        "# setting the batch size and the epochs\n",
        "\n",
        "batch_size = 8\n",
        "epochs = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59vUXpFlYZK-"
      },
      "source": [
        "Splitting the images (80% training and 20% \n",
        "validation) and Data augmanting it\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYggOThqYYN4"
      },
      "source": [
        "\n",
        "\n",
        "directory = 'data'\n",
        "\n",
        "img_datagen = ImageDataGenerator(validation_split=0.2,        # Splits the data into training (80%) and validation (20%)\n",
        "                                   rescale = 1./255,            # Multiple the colors by a number between 0-1 to process data faster\n",
        "                                   rotation_range=40,           #rotate the images\n",
        "                                   width_shift_range=0.2,       #fraction of the total width\n",
        "                                   height_shift_range = 0.2,    #fraction of the total height\n",
        "                                   zoom_range = 0.2,            #float percent\n",
        "                                   horizontal_flip=True,        #horizontal flip\n",
        "                                   fill_mode='nearest')        #add new pixels when the image is rotated or shifted\n",
        "\n",
        "train_generator = img_datagen.flow_from_directory(\n",
        "                                directory,\n",
        "                                target_size = (70, 70),\n",
        "                                batch_size = batch_size,\n",
        "                                color_mode=\"rgb\",               # for coloured images\n",
        "                                class_mode = 'binary',\n",
        "                                shuffle = True,\n",
        "                                seed=42,                      # to make the result reproducible\n",
        "                                subset = 'training')            # Specify this is training set\n",
        "\n",
        "validation_generator = img_datagen.flow_from_directory(\n",
        "                                directory,\n",
        "                                target_size = (70, 70),\n",
        "                                batch_size = batch_size,\n",
        "                                color_mode=\"rgb\",               # for coloured images\n",
        "                                class_mode = 'binary',\n",
        "                                subset = 'validation')            # Specify this is training set\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drYp_mO7ICUu"
      },
      "source": [
        "**Display a batch of the images used in the training and thier labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld4QAqg4IBi4"
      },
      "source": [
        "#generate a batch of images and labels from the training set\n",
        "imgs, labels = next(train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3AUD1pXIXb_"
      },
      "source": [
        "#plotting function\n",
        "\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, batch_size, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip( images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxt3aeyNIYU9"
      },
      "source": [
        "#displaying the images and thier labels where as 0 with mask and 1 without mask\n",
        "plotImages(imgs);\n",
        "print(labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG0mM8yggu0e"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "###**2. Build and train the CNN**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZvVgvRokJeJ"
      },
      "source": [
        "Sequencial is a list of the layers of the model we want to create. Here it consists of\n",
        "\n",
        "*   Conv2D Layer\n",
        "> * **The filter** parameter means the number of this layer's output filters which is less in the early layers and more when we are closer to the prediction, [recommended to start up with 32,64,128 and the number varies according to the depth of the model]\n",
        "> *   **The kernal_size** specifies the width and the height of the 2D convolution window [odd integer and depend on the image size if image size > 128x128 then use 5*5 if less use 3x3 or 1x1]\n",
        "> *   **The activation** parameter refers to the type of activation function\n",
        "> *   **The padding** parameter is enabled to zero-padding to preserve the spatial dimensions of the volume so the output volume size matches the input volume size\n",
        "> *   **The input_shape** parameter has pixel high and pixel wide and have the 3 color channels: RGB\n",
        "\n",
        "*   MaxPool2D Layer\n",
        "> To pool and reduce the dimensionlaity of the data\n",
        "> *   pool_size: max value over a 2x2 pooling window\n",
        "> *  strides: how far the pooling window moves for each pooling step\n",
        "*   Flatten Layer\n",
        "> * flatten is used to flatten the input to a 1D vector then passed to dense\n",
        "\n",
        "*   Dense Layer (The output layer)\n",
        "> * **The units** parameter means that it has 2 nodes one for with and one for without because we want a binary output \n",
        "> *   **The activation** parameter we use the softmax activation function on our output so that the output for each sample is a probability distribution over the outputs of with and without mask\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSFD3JDdN9Fr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573e0c20-12d6-4fd8-b679-280ff3e55a0d"
      },
      "source": [
        "#create the model layers\n",
        "#we have functional and sequential\n",
        "model = Sequential([\n",
        "                    Conv2D(filters=32, kernel_size=(3,3),activation='relu',padding='same',input_shape=(70,70,3)),\n",
        "                    MaxPool2D(pool_size=(2,2), strides=2),\n",
        "                    Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding= 'same'),\n",
        "                    MaxPool2D(pool_size=(2,2), strides =2),\n",
        "                    Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding= 'same'),\n",
        "                    MaxPool2D(pool_size=(2,2), strides =2),\n",
        "                    Flatten(),\n",
        "                    Dense(units=64, activation= 'relu'),\n",
        "                    #means the output is 0,1 (the labels) and the P(c=0) +P(c=1) = 1 \n",
        "                    Dense(units=1, activation='sigmoid'), \n",
        "\n",
        "])\n",
        "\n",
        "#check out the model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 70, 70, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 35, 35, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 35, 35, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 17, 17, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 17, 17, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                262208    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 290,913\n",
            "Trainable params: 290,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTdz4MtVTy7i"
      },
      "source": [
        "Compile the model using the **Adam** optimizer with **learning rate** of `0.0001`, a **loss** of `binary_crossentropy`, and we'll look at `accuracy` as our performance **metric**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSITIIHuTyBQ"
      },
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001),         # remsprop, adadelta\n",
        "              loss='binary_crossentropy',                   # mean_squared_error (regression task), categorical_crossentropy\n",
        "              metrics=['accuracy'])                         #metrices to keep track off"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERuAVVRaUvGk"
      },
      "source": [
        "We use the `train_generator` because we are now only training the data. The \n",
        "validation data is the `validation_generator` \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aC83morUtyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b415c41c-bd10-4562-e5ea-a9bb65ad71d2"
      },
      "source": [
        "#Training the model\n",
        "history = model.fit(train_generator ,epochs = epochs,validation_data= validation_generator, batch_size= batch_size) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1002/1002 [==============================] - 51s 18ms/step - loss: 0.5149 - accuracy: 0.7309 - val_loss: 0.3887 - val_accuracy: 0.8277\n",
            "Epoch 2/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.2986 - accuracy: 0.8770 - val_loss: 0.3997 - val_accuracy: 0.8242\n",
            "Epoch 3/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.2558 - accuracy: 0.8947 - val_loss: 0.2897 - val_accuracy: 0.8816\n",
            "Epoch 4/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.2233 - accuracy: 0.9134 - val_loss: 0.2569 - val_accuracy: 0.8966\n",
            "Epoch 5/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1874 - accuracy: 0.9311 - val_loss: 0.2857 - val_accuracy: 0.8856\n",
            "Epoch 6/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1716 - accuracy: 0.9350 - val_loss: 0.3619 - val_accuracy: 0.8641\n",
            "Epoch 7/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1551 - accuracy: 0.9403 - val_loss: 0.1934 - val_accuracy: 0.9336\n",
            "Epoch 8/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1446 - accuracy: 0.9483 - val_loss: 0.1911 - val_accuracy: 0.9251\n",
            "Epoch 9/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1311 - accuracy: 0.9528 - val_loss: 0.1528 - val_accuracy: 0.9446\n",
            "Epoch 10/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1332 - accuracy: 0.9527 - val_loss: 0.1581 - val_accuracy: 0.9381\n",
            "Epoch 11/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1223 - accuracy: 0.9560 - val_loss: 0.1884 - val_accuracy: 0.9316\n",
            "Epoch 12/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1183 - accuracy: 0.9593 - val_loss: 0.1222 - val_accuracy: 0.9535\n",
            "Epoch 13/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1118 - accuracy: 0.9580 - val_loss: 0.1219 - val_accuracy: 0.9560\n",
            "Epoch 14/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1050 - accuracy: 0.9594 - val_loss: 0.1434 - val_accuracy: 0.9466\n",
            "Epoch 15/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1034 - accuracy: 0.9624 - val_loss: 0.1171 - val_accuracy: 0.9565\n",
            "Epoch 16/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1022 - accuracy: 0.9626 - val_loss: 0.1142 - val_accuracy: 0.9610\n",
            "Epoch 17/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1130 - accuracy: 0.9600 - val_loss: 0.1195 - val_accuracy: 0.9555\n",
            "Epoch 18/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1031 - accuracy: 0.9650 - val_loss: 0.1216 - val_accuracy: 0.9575\n",
            "Epoch 19/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.1003 - accuracy: 0.9644 - val_loss: 0.1115 - val_accuracy: 0.9625\n",
            "Epoch 20/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0890 - accuracy: 0.9704 - val_loss: 0.1464 - val_accuracy: 0.9476\n",
            "Epoch 21/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0980 - accuracy: 0.9670 - val_loss: 0.0993 - val_accuracy: 0.9650\n",
            "Epoch 22/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0879 - accuracy: 0.9689 - val_loss: 0.1217 - val_accuracy: 0.9535\n",
            "Epoch 23/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0841 - accuracy: 0.9696 - val_loss: 0.1215 - val_accuracy: 0.9565\n",
            "Epoch 24/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0864 - accuracy: 0.9694 - val_loss: 0.0918 - val_accuracy: 0.9650\n",
            "Epoch 25/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0918 - accuracy: 0.9675 - val_loss: 0.1011 - val_accuracy: 0.9635\n",
            "Epoch 26/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0736 - accuracy: 0.9762 - val_loss: 0.1022 - val_accuracy: 0.9660\n",
            "Epoch 27/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0752 - accuracy: 0.9744 - val_loss: 0.0964 - val_accuracy: 0.9645\n",
            "Epoch 28/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0835 - accuracy: 0.9723 - val_loss: 0.0815 - val_accuracy: 0.9715\n",
            "Epoch 29/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0840 - accuracy: 0.9690 - val_loss: 0.0856 - val_accuracy: 0.9675\n",
            "Epoch 30/30\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.0789 - accuracy: 0.9736 - val_loss: 0.1007 - val_accuracy: 0.9635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0w7InFv_PoB"
      },
      "source": [
        "model.save('MaskCheck.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwuYilWn_QrZ"
      },
      "source": [
        "new_model = tf.keras.models.load_model('MaskCheck.model') #loading a model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsUnQyFnYDkS"
      },
      "source": [
        "\n",
        "\n",
        "### **3. Plotting the loss and accuracy of training vs validation**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "627OK11rYiYO"
      },
      "source": [
        "#Plotting the loss of validation and training \n",
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochstoplot = range(1,epochs+1)\n",
        "plt.plot(epochstoplot, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochstoplot, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSSt2DJZY5qK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "c4fa9915-3504-4783-edbe-f13b502a2bcf"
      },
      "source": [
        "#Plotting the accuracy of validation and training \n",
        "accur_train = history.history['accuracy']\n",
        "accur_val = history.history['val_accuracy']\n",
        "plt.plot(epochstoplot, accur_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochstoplot, accur_val, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7339359f771e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Plotting the accuracy of validation and training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccur_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maccur_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochstoplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccur_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochstoplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccur_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AugzFppxIbiY"
      },
      "source": [
        "### **4. Testing the CNN model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U-E1qlBcDif"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "TGREEN =  '\\033[1;37;42m'\n",
        "TRED =    '\\033[1;37;41m'\n",
        "for i in range (1,17):\n",
        "  img_directory = str(i) + '.jpg'\n",
        "  img_data = image.load_img(img_directory, target_size = (70, 70))   #load the image from the directory\n",
        "  img_data = image.img_to_array(img_data)                            #convert the image to a Numpy array\n",
        "  img_data = np.expand_dims(img_data, axis = 0)                     #expands the array by inserting a new axis at the specified position.\n",
        "\n",
        "  classify = model.predict(img_data)\n",
        "  display(Image(img_directory,width= 150, height=150))\n",
        "  print(\"\\n\")\n",
        "  if(int(classify[0][0]) == 0):\n",
        "    print(TGREEN + \"The person is wearing a mask. \\n\")\n",
        "  else:\n",
        "    print(TRED + \"The person is not wearing a mask.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}